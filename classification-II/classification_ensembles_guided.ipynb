{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Ensembles - Guided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Heart Disease UCI dataset from [Kaggle](https://www.kaggle.com/ronitf/heart-disease-uci) to explore how to implement ensemble classifiers using `sklearn.ensemble`. In this exercise we are not going go try and interpret out classifiers much (in general, ensemble methods obscure this somewhat anyway as we will discuss further in the Online Practice), and we are not going to deal with any other issues that might be present or strategies that might be available. We will simply explore the different ensemble methods available and see if we can use any to improve the classifier performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the usual libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the class balance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so we see that the data is fairly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick high-level check of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so it appears that we will we have no cleaning to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into features and class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split\n",
    "\n",
    "\n",
    "# perform a train/test split with a 70:30 ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import a range of different classifiers that we can try to use in our ensembles, some of which we have studied in detail, and some we have not! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a range of classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import various functions we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cross_val_score and GridSearchCV \n",
    "\n",
    "\n",
    "# import StandardScaler for certain algorithms\n",
    "\n",
    "\n",
    "# import make_pipeline to conveniently package up the scaler with certain algorithms\n",
    "\n",
    "\n",
    "# import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try constructing a **voting classifier** using a few different classifiers. Let's leave all of the model arguments as default for now - we will return to this point shortly.\n",
    "\n",
    "> In the following we will evaluate our model using `cross_val_score`, effectively like computing the accuracy on a validation set, as we may sometimes want to set hyperparameter values or choose between ensembles. As always, the final evaluation would eventually be on the test set at the very end of an analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VotingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a SVC, DecisionTreeClassifier and KNeighborsClassifier\n",
    "\n",
    "\n",
    "# construct a hard VotingClassifier using these learners\n",
    "\n",
    "\n",
    "# for each learner and the ensemble as a whole, use cross_val_score with 5 folds to gauge the accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not doing very well! It's not even performing as well as the best learner. This may seem contradictory to our original motivating example of the oracles, but remember that the underlying calculation there implicitly assumed that all learners are completely *independent*, which in reality can never be met.\n",
    "\n",
    "\n",
    "Let's try changing the voting strategy to \"soft\" and play with the weights a bit based on which individual learners above are performing better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a SVC, DecisionTreeClassifier and KNeighborsClassifier\n",
    "\n",
    "\n",
    "# construct a soft VotingClassifier using these learners, and set the weights\n",
    "\n",
    "\n",
    "# for each learner and the ensemble as a whole, use cross_val_score with 5 folds to gauge the accuracy\n",
    "for clf in (clf1, clf2,  clf3,  eclf):\n",
    "    scores = cross_val_score(clf, X_train, y_train, scoring = 'accuracy', cv = 5)\n",
    "    print(\"%s: %f \" % (clf.__class__.__name__, scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is certainly a bit better, and now our voting classifier is performing slightly better than the individual learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a different voting ensemble, using log reg rather than SVM, and this time let's correctly scale the data that k-NN uses (using the `make_pipeline` function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a LogisticRegression, DecisionTreeClassifier and KNeighborsClassifier\n",
    "\n",
    "\n",
    "# construct a hard VotingClassifier using these learners, and pipeline the k-NN with a scaler\n",
    "\n",
    "                       \n",
    "\n",
    "# for each learner and the ensemble as a whole, use cross_val_score with 5 folds to gauge the accuracy\n",
    "for clf in (clf1, clf2, make_pipeline(StandardScaler(), clf3), eclf):\n",
    "    scores = cross_val_score(clf, X_train, y_train, scoring = 'accuracy', cv = 5)\n",
    "    print(\"%s: %f \" % (clf.__class__.__name__, scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are getting some real improvement this time! Our voting classifier ensemble performs almost 2% better than the best learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Setting Hyperparameter Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have not tried to fix any of the hyperparameter values of the individual learners. We can take different approaches to choosing the individual learner hyperparameters which will not in general produce the same hyperparameter values,: \n",
    "\n",
    "1. You could first grid search the individual learners to find the best values before they get put into the ensemble \n",
    "2. You can grid search the ensemble as a whole - for this, you access the desired argument via a double underscore in the parameter grid as we will demonstrate below\n",
    "\n",
    "Let us take the second approach. For example, in the code below we grid search to find the `max_depth` of the decision tree, the `C` parameter of the log reg model, and the k-NN hyperparmaters `n_neighbors` and `metric`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a LogisticRegression, DecisionTreeClassifier and KNeighborsClassifier\n",
    "clf1 = LogisticRegression(max_iter = 10000)\n",
    "clf2 = DecisionTreeClassifier(random_state = 1)\n",
    "clf3 = KNeighborsClassifier()\n",
    "\n",
    "# construct a hard VotingClassifier using these learners, and pipeline the k-NN with a scaler\n",
    "eclf = VotingClassifier(estimators = [('lr', clf1), \n",
    "                                      ('dt', clf2), \n",
    "                                      ('knn', make_pipeline(StandardScaler(), clf3))\n",
    "                                     ],\n",
    "                                      voting = 'hard',\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameter grid, noting the double underscore notation to access elements of the ensemble\n",
    "\n",
    "\n",
    "# pass the voting classifier and the param_grid to a grid search and use 5 folds\n",
    "\n",
    "\n",
    "# fit the grid search to the training set\n",
    "\n",
    "\n",
    "# print the best parameter values\n",
    "\n",
    "\n",
    "# save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each learner and the ensemble as a whole, use cross_val_score with 5 folds to gauge the accuracy\n",
    "for clf in (clf1, clf2, make_pipeline(StandardScaler(), clf3), eclf):\n",
    "    scores = cross_val_score(clf, X_train, y_train, scoring = 'accuracy', cv = 5)\n",
    "    print(\"%s: %f \" % (clf.__class__.__name__, scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there isn't any real improvement over using the default arguments of the learners in the voting classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than trying to endlessly tweak weights etc., you can instead train a meta-classifier via **stacking**. Here we have to choose a classification algorithm for our final \"meta-classifier\", which uses the individual predictions from the various learners as inputs to another learning model.\n",
    "\n",
    "Let's use the same 3 individual learners from before, and try some different meta-classifiers via the `final_estimator` argument - in practice one would usually use something like a log reg or SVM model for this meta-classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import StackingClassifier\n",
    "\n",
    "\n",
    "# setup a LogisticRegression, DecisionTreeClassifier and KNeighborsClassifier\n",
    "\n",
    "\n",
    "# define the learners in the ensemble\n",
    "\n",
    "\n",
    "# create a stacking ensemble with a LogisticRegression meta-classifier\n",
    "\n",
    "\n",
    "# fit the ensemble to the training data \n",
    "\n",
    "\n",
    "# for each learner and the ensemble as a whole, use cross_val_score with 5 folds to gauge the accuracy\n",
    "for clf in (clf1, clf2,  make_pipeline(StandardScaler(), clf3), stack):\n",
    "    scores = cross_val_score(clf, X_train, y_train, scoring = 'accuracy', cv = 5)\n",
    "    print(\"%s: %f \" % (clf.__class__.__name__, scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it with a SVC meta-classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a LogisticRegression, DecisionTreeClassifier and KNeighborsClassifier\n",
    "clf1 = LogisticRegression(max_iter = 10000)\n",
    "clf2 = DecisionTreeClassifier(random_state = 123)\n",
    "clf3 = KNeighborsClassifier()\n",
    "\n",
    "# define the learners in the ensemble\n",
    "estimators = [('lr', clf1), \n",
    "              ('dt', clf2), \n",
    "             ('knn', make_pipeline(StandardScaler(), clf3))]\n",
    "\n",
    "# create a stacking ensemble with a SVC meta-classifier\n",
    "stack = StackingClassifier(\n",
    "    estimators = estimators,\n",
    "    final_estimator = SVC())\n",
    "\n",
    "# fit the ensemble to the training data \n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "# for each learner and the ensemble as a whole, use cross_val_score with 5 folds to gauge the accuracy\n",
    "for clf in (clf1, clf2,  make_pipeline(StandardScaler(), clf3), stack):\n",
    "    scores = cross_val_score(clf, X_train, y_train, scoring = 'accuracy', cv = 5)\n",
    "    print(\"%s: %f \" % (clf.__class__.__name__, scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So both of these are giving some improvement to our performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle you can use any classifier for the meta-classifier here, and in fact you could choose between these options for the `final_estimator` by using `GridSearchCV` if you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than simply collecting a few learners into an ensemble, in practice we often like to use *many* learners in an ensemble - one of the most common ways to do this is via **bagging**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that bagging reduces the variance (without increasing bias) i.e. it can help reduce overfitting. Bagging performs best with algorithms that have *high variance*, such as unpruned decision trees - you saw in the Introduction for Classification I that decision trees, as non-parametric models, can easily overfit if left unrestrained.\n",
    "\n",
    "The main arguments to play with here are `base_estimator`, `max_samples`, and `n_estimators`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a decision tree model with default arguments\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# fit the tree to the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# use cross_val_score to compute the accuracy of the model with 5 folds\n",
    "cross_val_score(model, X_train, y_train, scoring = 'accuracy', cv = 5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try bagging some decision trees together - let's construct an ensemble of 500 trees, each one only seeing a random 0.6 of the dataset (for extra diversity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BaggingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a bagging ensemble with 500 trees, each one only seeing 0.6 of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross_val_score to compute the accuracy of the ensemble with 5 folds\n",
    "cross_val_score(model, X_train, y_train, scoring = 'accuracy', cv = 5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is a significant increase compared to a single tree - about 5%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, this works best for high variance models like trees. Let's see how it performs on log reg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a log reg model with default arguments (except set max_iter = 10000)\n",
    "model = LogisticRegression(max_iter = 10000)\n",
    "\n",
    "# fit the model to the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# use cross_val_score to compute the accuracy of the model with 5 folds\n",
    "cross_val_score(model, X_train, y_train, scoring = 'accuracy', cv = 5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a bagging ensemble with 500 models, each one only seeing 0.6 of the dataset\n",
    "bag = BaggingClassifier(base_estimator = LogisticRegression(),\n",
    "                        max_samples = 0.6, n_estimators = 500, n_jobs=-1, random_state = 123)\n",
    "\n",
    "# use cross_val_score to compute the accuracy of the ensemble with 5 folds\n",
    "cross_val_score(model, X_train, y_train, scoring = 'accuracy', cv = 5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And on Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a GaussianNB model with default arguments \n",
    "model = GaussianNB()\n",
    "\n",
    "# fit the model to the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# use cross_val_score to compute the accuracy of the model with 5 folds\n",
    "cross_val_score(model, X_train, y_train, scoring = 'accuracy', cv = 5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a bagging ensemble with 500 models, each one only seeing 0.6 of the dataset\n",
    "\n",
    "bag = BaggingClassifier(base_estimator = GaussianNB(),\n",
    "                        max_samples = 0.6, n_estimators = 500, n_jobs = -1, random_state = 123)\n",
    "\n",
    "cross_val_score(model, X_train, y_train, scoring = 'accuracy', cv = 5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for some other classification algorithms there is not much of a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get the best accuracy score on the dataset, using any of these techniques including voting classifiers, bagging or stacking. You can use any classification algorithm - `LogisticRegression`, `DecisionTreeClassifier`, `SVC`, `KNeighborsClassifier`, `GaussianNB`, or even other ensembles like `RandomForestClassifier` as learners in your ensemble. You can try to optimise the hyperparameters or keep things simple.\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
