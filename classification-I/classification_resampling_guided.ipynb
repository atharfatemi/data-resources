{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Resampling Techniques for Imbalanced Data - Guided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced data is a very common issue that can sometimes cause huge problems for classifier performance. By \"imbalanced\", we mean that one or more classes is significantly under-represented in the dataset. Sometimes this is fully expected given the domain of the problem at hand - for example, if you were analysing a transaction dataset looking for credit card fraud, you would probably expect to see *many* more instances labelled \"not fraud\" than \"fraud\".\n",
    "\n",
    "Unfortunately, there is no golden gun solution to this problem, but various different strategies have been developed for improving model performance in such cases. You can simply try different algorithms as some are better suited to imbalanced data, you can extend algorithms to assign distinct costs to training examples and thereby give different weights to classes, you can try ensemble methods, or you can try manipulating the datasets itself.\n",
    "\n",
    "At the data level in particular, there are a few **resampling** techniques that try to balance the classes a little better. We will discuss these in this notebook, as well as their implementation in `imbalanced-learn` or `imblearn`. We will then return to the customer churn problem we tackled in the Introduction work (which used an imbalanced dataset) and see some of these ideas in practice - along the way we will introduce the concept of ROC curves, which are a powerful tool commonly used in classifier evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Data: Creating an Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the usual libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To synthesise some data to illustrate these techniques, we can use the `sklearn.datasets.make_classification` class - this is a fairly simple construction that produces normally distributed clusters some specified distance apart, which you can read about in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) if you are interested. \n",
    "\n",
    "Let's create a binary dataset with 10000 data points, 2 features, a 99:1 class imbalance, and some slight noise (set via the `flip_y` argument) so that the dataset is not *perfectly* linearly separable (in which case an imbalance might not matter much anyway):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import make_classification\n",
    "\n",
    "# construct a binary classification dataset with 2 features, 10000 instances, a 99:1 ratio and slight noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we also set the `random_state` argument above as, without this, the above code will produce quite a different dataset each time it runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function outputs X and y as `np.array`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check the class imbalance by converting y to a `pd.Series` and using `value_counts()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the class imbalance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine features and class into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine X and y into a pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the head of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the class variable as 'class'\n",
    "df = df.rename(columns = {0: 'class'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the head of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our data on a scatterplot, with `hue` given by the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a scatterplot of the 2 features with hue given by the class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first resampling technique we will demonstrate is called **random undersampling** - this consists of simply reducing the instances in the majority class (which for us is class 1 - \"positive\" class) by randomly removing some instances, thereby balancing the dataset to a certain degree.\n",
    "\n",
    "> All of the resampling techniques we will use in this notebook have implementations in the `imbalanced-learn` or `imblearn` library (which is somewhat modelled after `sklearn`) - if you do not currently have it installed, you should install it using pip via a command like `pip install imbalanced-learn`.\n",
    "\n",
    "Let's implement random undersampling using `RandomUnderSampler` and see how it affects our data. The main argument we want to set is the `sampling_strategy` argument which can take on a number of values as described in the [documentation](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.RandomUnderSampler.html#imblearn.under_sampling.RandomUnderSampler) - for us, we will just set it to a float which will represent the ratio of minority to majority classes after the resampling. All resampling techniques have their own issues, and so you are unlikely to use them to completely balance the classes in practice - we will pick ratio values that are somewhere in the middle throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the function\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# create a RandomUnderSampler that will finish with a 10:1 ratio of classes, and fix the random state\n",
    "\n",
    "\n",
    "# fit the resampler to our dataset and store it into new variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now reproduce the previous steps of converting this data into a `pandas` dataframe and producing a scatterplot using `seaborn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to a dataframe, rename the target as \"class\", and plot a scatterplot\n",
    "df = pd.DataFrame(data = X_undersamp, columns = [\"feature_1\", \"feature_2\"])\n",
    "df = pd.concat([df, pd.Series(y_undersamp)], axis = 1)\n",
    "df = df.rename(columns = {0: 'class'})\n",
    "\n",
    "sns.scatterplot(x = df['feature_1'], y = df['feature_2'], hue = df['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is hopefully clear, the majority class (blue) has fewer instances compared to before, so much so in fact that we can actually see some minority class (orange) instances poking through in the middle of the blue cluster, which were hidden before. \n",
    "\n",
    "We can check that this has indeed left the minority class alone and simply reduced the majority class by checking the class imbalance again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of undersampling is that, especially for large datasets, it can reduce both processing time and storage demands by reducing the number of training examples. However, the main issue with this is that instances might be discared that contain potentially useful information for building the classifiers - the remaining sample is chosen randomly, and so there is no guarantee that it is a representative sample of the population, which may lead to bad generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A contrasting approach is known as **random oversampling** - this randomly duplicates certain instances in the minority class, in order to try and balance the classes a bit. We can implement this using `RandomOverSampler`, which takes the same arguments as `RandomUnderSampler` as you can see in the [documentation](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.RandomOverSampler.html#imblearn.over_sampling.RandomOverSampler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the function\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# create a RandomOverSampler that will finish with a 10:1 ratio of classes, and fix the random state\n",
    "\n",
    "\n",
    "# fit the resampler to our dataset and store it into new variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to a dataframe, rename the target as \"class\", and plot a scatterplot\n",
    "df = pd.DataFrame(data = X_oversamp , columns = [\"feature_1\", \"feature_2\"])\n",
    "df = pd.concat([df, pd.Series(y_oversamp)], axis = 1)\n",
    "df = df.rename(columns = {0: 'class'})\n",
    "\n",
    "sns.scatterplot(x = df['feature_1'], y = df['feature_2'], hue = df['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the majority class has not been reduced at all, but some minority class instances have been duplicated - so much so that again we can see some poking through in the middle of majority class cluster in the plot above now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that oversampling has left the majority class alone, and has simply increased the number of minority class instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of oversampling is that, unlike undersampling, this does not discared of any training instances and therefore does not risk losing valuable information for classifier building - for this reason it often performs better than undersampling. However, the duplication of minority class instances does increase the likelihood of *overfitting* and so runs the risk of poor generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Minority Oversampling Technique (SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated oversampling technique than simple random oversampling was introduced in a 2002 [paper](https://arxiv.org/pdf/1106.1813.pdf) and utilises the idea of constructing new *synthetic* training examples for the minority class - for this reason the technique is called **synthetic minority oversampling technique (SMOTE)**.\n",
    "\n",
    "SMOTE creates new synthetic minority instances by drawing *straight lines* between a minority instance and a certain number of nearest minority instance neighbours in feature space, and randomly placing new instances along one of these lines. We can implement it using `SMOTE` which again takes the same arguments as the previous two classes we have utilised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the function\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# create a SMOTE resampler that will finish with a 10:1 ratio of classes, and fix the random state\n",
    "\n",
    "\n",
    "# fit the resampler to our dataset and store it into new variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to a dataframe, rename the target as \"class\", and plot a scatterplot\n",
    "df = pd.DataFrame(data = X_smote, columns = [\"feature_1\", \"feature_2\"])\n",
    "df = pd.concat([df, pd.Series(y_smote)], axis = 1)\n",
    "df = df.rename(columns = {0: 'class'})\n",
    "\n",
    "sns.scatterplot(x = df['feature_1'], y = df['feature_2'], hue = df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with random oversampling, this does not risk losing valuable information, and moreover it reduces the risk of overfitting since it generates new synthetic examples rather than merely duplicating existing instances. However, the main problem is that while generating synthetic examples SMOTE does not take into consideration neighboring examples from other classes -  this can result in increase in overlapping of classes and can introduce additional noise as we can see from the above plot. SMOTE is also not very effective for high dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original [paper](https://arxiv.org/pdf/1106.1813.pdf) introducing SMOTE, it was suggested that SMOTE be preceeded by undersampling to first trim away some majority class instances - it was argued that this combination produced the best classifier performance. This can be efficiently implemented using `imblearn.pipeline` (which is modelled after `sklearn.pipeline` as will be briefly mentioned in the Online Practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the function\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# set up the undersampler and SMOTE oversampler such that we end up with a 10:1 ratio\n",
    "\n",
    "\n",
    "# construct the resampling pipeline\n",
    "\n",
    "\n",
    "# apply the resampling pipeline to the data and store into new variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to a dataframe, rename the target as \"class\", and plot a scatterplot\n",
    "df = pd.DataFrame(data = X_pipe , columns = [\"feature_1\", \"feature_2\"])\n",
    "df = pd.concat([df, pd.Series(y_pipe)], axis = 1)\n",
    "df = df.rename(columns = {0: 'class'})\n",
    "\n",
    "sns.scatterplot(x = df['feature_1'], y = df['feature_2'], hue = df['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again check the class imbalance as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we have ended up with less training data here, but the above plot is perhaps a more reasonable representation than that produced by SMOTE alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning to our Customer Churn Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall our customer churn problem that we tackled in the Introduction work. Let's quickly reproduce the loading of data, cleaning, and preprocessing that performed there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "data = pd.read_csv(\"churn.csv\")\n",
    "\n",
    "# drop unhelpful columns\n",
    "data = data.drop(['CustomerId', 'Surname', 'RowNumber'], axis = 1)\n",
    "\n",
    "# make class names clearer\n",
    "data['Exited'] = data['Exited'].replace([0,1], ['No', 'Yes'])\n",
    "\n",
    "# split into features (X) and class (y)\n",
    "X = data.drop('Exited', axis = 1)\n",
    "y = data['Exited']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# perform a train/test split using a 70:30 ratio and fixing the random seed to e.g. 123\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 123)\n",
    "\n",
    "# perform one-hot encoding on X_train to handle categorical features\n",
    "X_train = pd.get_dummies(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the train/test split has roughly produced the correct proportions of classes in the train and test sets (for several imbalanced datasets this might not be the case, and you might need to try a **stratified** split instead):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So indeed in this case we see that the class proportions are roughly preserved in the train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to rebuild our decision tree model and evaluate as we did in the Introduction work. As discussed previously, accuracy is not an appropriate measure to use for imbalanced data - other metrics like precision, recall, and f1-score are more suited. Another standard in classifier evaluation that is often used is given by the **ROC AUC** as we will now discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are studying a binary classification problem with 2 classes that are labelled \"Yes\" and \"No\". Recall that a prediction can be described as:\n",
    "\n",
    "True positive: predicts \"Yes\" when actually \"Yes\"  \n",
    "True negative: predicts \"No\" when actually \"No\"  \n",
    "False positive: predicts \"Yes\" when actually \"No\"  \n",
    "False negative: predicts \"No\" when actually \"No\"  \n",
    "\n",
    "From this we can define the **true positive rate (TPR)**:\n",
    "\n",
    "```TPR = # true positives / # positives```\n",
    "\n",
    "and **false positive rate (FPR)**:\n",
    "\n",
    "```FPR = # false positives / # negatives```\n",
    "\n",
    "Clearly, we would ideally like our classifier to have a high TPR and a low FPR. This can be conveniently assessed visually from a **receiver operating characteristic (ROC) curve**, which is a plot of the TPR against the FPR for each possible value of the decision threshold (recall that this is the probability threshold used to decide whether or not an instance is in class A or class B, and *need* not always be 0.5). The ROC curve can be used to select the optimal choice of classification threshold, by finding the sweet spot where the TPR is high and the FPR is low, as we will demonstrate in the Online Practice.\n",
    "\n",
    "Every ROC curve starts in the bottom-left and moves to the top-right of the curve. Why is this? For a classification threshold value of O, every \"Yes\" value is classified as \"Yes\" - thus, the TPR is 1. However, all the \"No\" values are also classified as \"Yes\", and so the FPR is also 1. As the threshold value increases, more and more \"No\" labels are assigned. Thus, the FPR and TPR both decrease - when the threshold reaches 1, no \"Yes\" labels are assigned, and the FPR and TPR are both 0. A high-performing classifier would have the TPR rapidly shoot up towards 1 across the lower range of FPR values - the closer the ROC curve traces along the vertical axis and approaches the upper-left hand of the plot, the better the classifier performs, since you will be able to find a good classification threshold (with high TPR and low FPR) as mentioned above. Thus, a useful metric to assess classifier performance in general is to compute the **area under the ROC curve (AUC)** - note that the theoretical maximum for the AUC is 1 since the TPR and FPR can range between 0-1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For imbalanced datasets, the **precision-recall curve** is actually usually favoured to the ROC curve as the latter can give a slightly optimistic view of performance, mainly because the ROC curve is fairly insensitive to class balance - we will use the latter for now however, as the AUC gives a convenient way for us to measure the classifier performance whilst cross-validating (or at least, a better measure than the accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can all be implemented using a couple of functions from the `sklearn.metrics` library. In particular, `roc_curve` computes the TPR and FPR for a variety of thresholds (which could then be plotted), and `auc` computes the area under the ROC curve. Let's illustrate it with a `LogisticRegression` classifier, a simple type of classifier you will learn a lot about in the Online Practice - we can setup and train the model in the usual `sklearn` way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LogisticRegression\n",
    "\n",
    "\n",
    "# setup the model\n",
    "\n",
    "\n",
    "# fit the model to the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import roc_curve and auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the test set too\n",
    "\n",
    "\n",
    "# use the model to simply generate the raw probabilities \n",
    "\n",
    "\n",
    "# extract the probabilities for the second class (\"Yes\")\n",
    "\n",
    "\n",
    "# given the probabilities and the true values, you can generate the roc_curve\n",
    "\n",
    "\n",
    "# compute the AUC\n",
    "\n",
    "\n",
    "# plot the roc_curve with the AUC value in the legend\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our Imbalanced Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stick with a `LogisticRegression` classifier for now, since it doesn't require the extra complexity of grid-searching, and is known to handle imbalanced data badly so we will hopefully see a real improvement from resampling. We have seen the ROC curve for the original model above - we can further evaluate the model via the standard metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import all the usual metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate all of the metrics on the test set\n",
    "print(\"Original AUC: \", roc_auc)\n",
    "print(\"Original accuracy: \", accuracy_score(y_test, model.predict(X_test)))\n",
    "print(\"Original precision: \", precision_score(y_test, model.predict(X_test), pos_label = 'Yes'))\n",
    "print(\"Original recall: \", recall_score(y_test, model.predict(X_test), pos_label = 'Yes'))\n",
    "print(\"Original f1: \", f1_score(y_test, model.predict(X_test), pos_label = 'Yes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the confusion matrix\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy, and to some extent the AUC, are overly optimistic for imbalanced data but the other metrics give away the underlying issue - this classifier is performing terribly on the other metrics demonstrating how badly the model is performing for the \"Yes\" class, and as we can see from the confusion matrix, it is in fact making very few \"Yes\" predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try our resampling combination of undersampling and SMOTE. Note that we never resample the **test set** that we are evaluating our model on - this would change the balance of classes, which is unrepresentative of new data that our classifier will be predicting on when it is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the undersampler and SMOTE oversampler\n",
    "under = RandomUnderSampler(sampling_strategy = 0.4, random_state = 1)\n",
    "over = SMOTE(sampling_strategy = 0.8, random_state = 1)\n",
    "\n",
    "# construct the pipeline\n",
    "steps = [('u', under), ('o', over)]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "# reample the data using the pipeline\n",
    "X_pipe, y_pipe = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# fit the model to the resampled data\n",
    "model.fit(X_pipe, y_pipe)\n",
    "\n",
    "# use the model to simply generate the raw probabilities \n",
    "y_probs = model.predict_proba(X_test)\n",
    "\n",
    "# extract the probabilities for the second class (\"Yes\")\n",
    "preds = y_probs[:,1]\n",
    "\n",
    "# given the probabilities and the true values, you can generate the roc_curve\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds, pos_label = 'Yes')\n",
    "\n",
    "# compute the AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plot the roc_curve with the AUC value in the legend\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again evaluate all of the metrics on the test set\n",
    "print(\"Resampled AUC: \", roc_auc)\n",
    "print(\"Resampled accuracy: \", accuracy_score(y_test, model.predict(X_test)))\n",
    "print(\"Resampled precision: \", precision_score(y_test, model.predict(X_test), pos_label = 'Yes'))\n",
    "print(\"Resampled recall: \", recall_score(y_test, model.predict(X_test), pos_label = 'Yes'))\n",
    "print(\"Resampled f1: \", f1_score(y_test, model.predict(X_test), pos_label = 'Yes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the confusion matrix\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than a slight drop in accuracy, this leads to a dramatic improvement in performance on the whole (though clearly still far from perfect performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now repeat this analysis using decision trees, like we originally did in the Introduction work. Decision trees are actually inherently better at handling imbalanced data, but we can check to see if we can still get any model improvement.\n",
    "\n",
    "Strictly speaking, we should repeat the grid search to choose our hyperparameter values when performing resampling, since the dataset will be altered. This introduces some extra complexity, since it is preferrable to use **stratified** k-folds when cross-validating, so that the class balances are preserved in each fold - we leave this as optional stretch material. For now, we will simply choose the \"best\" tree we found from the Introduction, which you might recall had `criterion = 'gini'` and `max_depth = 5`, and see if resampling improves the performance at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the tree model with critetion = 'gini' and max_depth = 5\n",
    "model = DecisionTreeClassifier(criterion = 'gini', max_depth = 5)\n",
    "\n",
    "# fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# generate the probabilities \n",
    "y_probs = model.predict_proba(X_test)\n",
    "\n",
    "# extract the probabilities for the second class (\"Yes\")\n",
    "preds = y_probs[:,1]\n",
    "\n",
    "# generate the roc_curve\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds, pos_label = 'Yes')\n",
    "\n",
    "# compute the AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# compute the metrics on the test set\n",
    "print(\"Original AUC: \", roc_auc)\n",
    "print(\"Original accuracy: \", precision_score(y_test, model.predict(X_test), pos_label = 'Yes'))\n",
    "print(\"Original precision: \", precision_score(y_test, model.predict(X_test), pos_label = 'Yes'))\n",
    "print(\"Original recall: \", recall_score(y_test, model.predict(X_test), pos_label = 'Yes'))\n",
    "print(\"Original f1: \", f1_score(y_test, model.predict(X_test), pos_label = 'Yes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's repeat our resampling pipeline and evaluate again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the undersample and SMOTE oversampler\n",
    "under = RandomUnderSampler(sampling_strategy = 0.3, random_state = 1)\n",
    "over = SMOTE(sampling_strategy = 0.6, random_state = 1)\n",
    "\n",
    "# construct the pipeline\n",
    "steps = [('u', under), ('o', over)]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "# run the pipeline\n",
    "X_pipe, y_pipe = pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "# setup the model\n",
    "model = DecisionTreeClassifier(criterion = 'gini', max_depth = 5)\n",
    "\n",
    "# fit the model to the resampled data\n",
    "model.fit(X_pipe, y_pipe)\n",
    "\n",
    "# calculate the probabilities\n",
    "y_probs = model.predict_proba(X_test)\n",
    "\n",
    "# extract the probabilities for the second class\n",
    "preds = y_probs[:,1]\n",
    "\n",
    "# generate the roc_curve\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds, pos_label = 'Yes')\n",
    "\n",
    "# compute the AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(\"Resampled AUC: \", roc_auc)\n",
    "print(\"Resampled accuracy: \", accuracy_score(y_test, model.predict(X_test)))\n",
    "print(\"Resampled precision: \", precision_score(y_test, model.predict(X_test), pos_label = 'Yes'))\n",
    "print(\"Resampled recall: \", recall_score(y_test, model.predict(X_test), pos_label = 'Yes'))\n",
    "print(\"Resampled f1: \", f1_score(y_test, model.predict(X_test), pos_label = 'Yes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look it at like this we see that the classifier is performing quite differently: \n",
    "\n",
    "- The AUC is about the same\n",
    "- The precision has dropped by about 10%\n",
    "- The accuracy has improved by about 5%\n",
    "- Notably, the recall has come up by about 13%\n",
    "- Importantly, the f1-score has also come up by about 6%\n",
    "\n",
    "There is no such thing as a free lunch, and these resampling techniques don't just solve the problem of imbalanced data for you - as expected from what we saw in the Introduction work, increasing the recall generally leads to a reduction in precision and vice versa, in line with the **precision-recall tradeoff** that we have mentioned previously. Depending on the case and the data at hand, these techniques can often improve classifier performance in the way that might matter to you for the problem you are solving, and so they are worth trying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Grid-searching with Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the `GridSearchCV` we did in the Introduction work to find the hyperparameters, but with one slight difference. Rather than simply creating k-folds randomly, we will create **stratified** folds so that the class proportions are maintained in each fold. This can be implemented using the `RepeatedStratifiedKFold` class from `sklearn` which can then be passed as the `cv` argument in `GridSearchCV`, rather than passing a simple integer as we did before (to specify the number of folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameter grid\n",
    "param_grid = [\n",
    "        {'criterion': ['gini', 'entropy'], \n",
    "         'max_depth': [2, 5, 7, 9, 11]},\n",
    "]\n",
    "\n",
    "# setup the model with all arguments as default\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\n",
    "\n",
    "# pass the model, cv, and the param_grid to the grid search\n",
    "grid_search = GridSearchCV(model, param_grid, cv = cv, scoring = 'roc_auc')\n",
    "\n",
    "# fit the grid search to the training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# return the hyperparameter values of the best model\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean score (AUC) of best estimator from cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it performs on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_test = pd.get_dummies(X_test)\n",
    "y_probs = model.predict_proba(X_test)\n",
    "\n",
    "# extract the probabilities for the second class\n",
    "preds = y_probs[:,1]\n",
    "\n",
    "# generate the roc_curve\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds, pos_label = 'Yes')\n",
    "\n",
    "# compute the AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(\"Test set AUC: \", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so there is very similar performance on train and test. We can summarise the classifier performance more fully by looking at the precision, recall and f1-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original AUC: \", roc_auc)\n",
    "print(\"Original accuracy: \", precision_score(y_test, model.predict(X_test), pos_label = 'Yes'))\n",
    "print(\"Original precision: \", precision_score(y_test, model.predict(X_test), pos_label = 'Yes'))\n",
    "print(\"Original recall: \", recall_score(y_test, model.predict(X_test), pos_label = 'Yes'))\n",
    "print(\"Original f1: \", f1_score(y_test, model.predict(X_test), pos_label = 'Yes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try our combination of undersampling and SMOTE. Note that we never resample the **test set** that we are evaluating our model on - this would change the balance of classes, which is unrepresentative of new data that our classifier will be predicting on when it is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under = RandomUnderSampler(sampling_strategy = 0.3, random_state = 1)\n",
    "over = SMOTE(sampling_strategy = 0.6, random_state = 1)\n",
    "\n",
    "steps = [('u', under), ('o', over)]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "X_pipe, y_pipe = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now repeat the grid search with our resampled training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameter grid\n",
    "param_grid = [\n",
    "        {'criterion': ['gini', 'entropy'], \n",
    "         'max_depth': [2, 5, 7, 9, 11]},\n",
    "]\n",
    "\n",
    "# setup the model with all arguments as default\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\n",
    "\n",
    "# pass the model and the param_grid to the grid search\n",
    "grid_search = GridSearchCV(model, param_grid, cv = cv, scoring = 'roc_auc')\n",
    "\n",
    "# fit the grid search to the training set\n",
    "grid_search.fit(X_pipe, y_pipe)\n",
    "\n",
    "# return the hyperparameter values of the best model\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean score (AUC) of best estimator from cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so see that this score has gone up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it performs on the **test set** however (which recall is totally \"new\" data that was not resampled at all, and so retains the original class imbalance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_pipe, y_pipe)\n",
    "\n",
    "X_test = pd.get_dummies(X_test)\n",
    "y_probs = model.predict_proba(X_test)\n",
    "\n",
    "# extract the probabilities for the second class\n",
    "preds = y_probs[:,1]\n",
    "\n",
    "# generate the roc_curve\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds, pos_label = 'Yes')\n",
    "\n",
    "# compute the AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(\"Test set AUC: \", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in fact the performance is very similar from the point of view of the AUC. We can see exactly how our classifier is performing differently now by looking at the various metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resampled AUC: \", roc_auc)\n",
    "print(\"Resampled accuracy: \", accuracy_score(y_test, model.predict(X_test)))\n",
    "print(\"Resampled precision: \", precision_score(y_test, model.predict(X_test), pos_label = 'Yes'))\n",
    "print(\"Resampled recall: \", recall_score(y_test, model.predict(X_test), pos_label = 'Yes'))\n",
    "print(\"Resampled f1: \", f1_score(y_test, model.predict(X_test), pos_label = 'Yes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a similar story to what we found with the best tree from the Introduction work that we used before - a slight drop in precision, but an increase in recall and f1-score.\n",
    "\n",
    "**Ensemble methods**, which you will learn about in your next Classification module, are another way to improve performance for imbalanced data, and sometimes a combination of resampling techniques and ensemble methods are the best strategy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
